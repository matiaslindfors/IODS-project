---
title: "Chapter4"
author: "Matias Lindfors"
date: "22 marraskuuta 2018"
output: html_document
---

```{}

# 1) Here's the assignment for week 4

# 2) Let's first download the Boston data:

install.packages("MASS")
library(MASS)
data(Boston)
str(Boston)
dim(Boston)

#The dataset includes 506 observations across 14 variables. The data contains housing data from the Boston area, including variables such as crime rate (crim), distance to employment centers (dis), and pupil-teacher ratio (ptratio).

# 3) Let's have a look at a summary of the included variables:

summary(Boston)

#The summary suggests that crime rates range from 0.00632 to 88.97620 per capita (IQR 0.08-3.67), and that the pupil-teacher ratio ranges from 12.6 to 22 (IQR 17.4-20.2), for instance.

#Let's visualise relationships (correlations) between variables in the data:

install.packages("corrplot")
library(corrplot)
cor_matrix <- cor(Boston)
corrplot(cor_matrix, method="circle", type="upper")

#The plot displays positive correlations in blue and negative correlations in red. Color intensity and the size of the circle are proportional to the correlation coefficients.
#Strongest positive correlations are found between accessibility to highways (rad) and property tax-rate (tax), and between air nitroxen oxide (nox) and industrial acres in town (indus).
#Strongest negative correlations are found between property value (medv) and proportion of low socioeconomic population (lstat), and between distance to employment center (dis) and air nitroxen oxide (nox).

# 4) Let's standardize the dataset:

Boston_scaled <- scale(Boston)
summary(Boston_scaled)

#After standardizing the data, the mean for all variables is now 0.00, as expected.

#Let's convert the scaled data from a matrix format to a data frame format:

Boston_scaled <- as.data.frame(Boston_scaled)

#Let's create a categorical variable "crime" of the crime rate variable (crim):

summary(Boston_scaled$crim)
bins <- quantile(Boston_scaled$crim)
crime <- cut(Boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

#Let's remove the crim-variable from the original data:

Boston_scaled <- dplyr::select(Boston_scaled, -crim)
dim(Boston_scaled)

#We now have 13 insted of 14 variables, good! Let's now add the new crime-variable to the scaled dataset:

Boston_scaled <- data.frame(Boston_scaled, crime)
dim(Boston_scaled)
str(Boston_scaled)

#...and we're back to 14 variables, with a categorical crime-variable, as can clearly be seen from analysing the structure of Boston_scaled.

#Let's divide the dataset to train (80%) and test (20%) sets:

nrow(Boston_scaled)
n <- nrow(Boston_scaled)
ind <- sample(n, size = n*0.8)
train <- Boston_scaled[ind, ]
test <- Boston_scaled[-ind, ]

# 5) Linear discriminant analysis:

lda.fit <- lda(crime ~ ., data = train)
lda.fit

#Drawing LDA biplot:

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

# 6) Predicting classes using the LDA model on the test data:

#Let's first save the crime-categories from the test set as "correct_classes", so that we can compare the predicted results with them later on:

correct_classes <- test$crime

#Removing the crime-variable from the test dataset:

test <- dplyr::select(test, -crime)

#Predicting the classes and crosstabulating the results:

lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)

#The table demonstrates that the LDA model predicted too low a class for 12 cases and too high a class for 23 cases. Most cases whose classes were wrongly predicted were in the "med_low" and "med_high" categories, but the predictive performance was quite good in the "low" and "high" categories (12/14 and 21/21 accurately predicted, respectively).

# 7) Clustering:

#Let's first reload the Boston dataset:

library(MASS)
data(Boston)
Boston2 <- Boston

#Standardizing the dataset:

Boston2_scaled <- scale(Boston2)

#Calculating (Euclidean) distances between observations:

dist_eu <- dist(Boston2_scaled)
summary(dist_eu)

#Running the kmeans algorithm, starting with 4 centers:

km <- kmeans(Boston2_scaled, centers = 4)
pairs(Boston2_scaled, col = km$cluster)

#This is all very well. However, let's now investigate what the optimal number of clusters is. Let's set the number of clusters a maximum of 10:

k_max <- 10

#Let's calculate the total within cluster sum of squares (twcss):

twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

#Let's visualize twcss:

install.packages("ggplot2")
library(ggplot2)
qplot(x = 1:k_max, y = twcss, geom = "line")

#As we've learned, we know that the number of clusters is optimal when the twcss is at its smallest. Our plot clearly demonstrates that twcss drastically reduces when k=2, meaning that 2 clusters will serve us quite fine!

#Now we can re-run the kmeans algorithm:

km <- kmeans(Boston2_scaled, centers = 2)
pairs(Boston2_scaled, col = km$cluster)

#We end up with a rather complex-looking plot. Regrettably, I can't interpret this. Let's instead move on to the bonus-assignment:

# 8) BONUS:

#Standardizing data:

library(MASS)
data(Boston)
Boston3 <- Boston
Boston3_scaled <- scale(Boston3)
Boston3_scaled <- data.frame(Boston3_scaled)

#Running kmeans using 4 clusters:

km3 <- kmeans(Boston3_scaled, centers = 4)

#Performing LDA using clusters as target classes:

lda.fit3 <- lda(km3$cluster ~., data = Boston3_scaled)

#Visualisation:

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
plot(lda.fit3, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit3, myscale = 1)

#Once again, we end up with quite a graph! Judging by arrow size, the variables "rad" and "indus" seem to be most influencial linear separators for the clusters.

#I'm afraid the super-bonus-assignment is out of my reach.

#THE END.

```
